{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Computer Vision subteam, we're excited to have you!\n",
    "\n",
    "As one of the most rapidly growing fields of engineering & computer science, there's plenty to explore and learn. \n",
    "\n",
    "First and foremost however, what is computer vision? \n",
    "\n",
    "Wikipedia says, \"Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.\" \n",
    "\n",
    "Personally, I'd define it as the process of designing *quantitative* algorithms for computers to match our *qualitative* understanding of the world. For example, I know that the image below is a coffee cup sitting on a table, and I know how far I'd need to move my hand to grab the cup. But how do we design an algorithm so that a computer can understand the same thing?\n",
    "\n",
    "\n",
    "<img src=\"images/coffee_cup.png\" alt=\"image description\" width=\"300\">\n",
    "\n",
    "For a quick video overview of the field, check out this video [from Nvidia.](https://www.youtube.com/watch?v=OnTgbN3uXvw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of this Notebook\n",
    "\n",
    "Through this notebook, you'll get an introduction to some of the main software libraries we use, as well as some key concepts.\n",
    "\n",
    "The first section of this notebook features a range of functions commonly used within image processing, with the second section of the notebook providing a section to play around with these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we'll import our neccessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CV2: Open-CV, a comprehensive computer vision library that's the core of many python computer vision pipelines.\n",
    "* Numpy: An array library that provides essential matrix operations and data structures. Typically, images will converted to numpy arrays while being processed.\n",
    "* Torch: PyTorch, a machine learning library that provides functions for creating, training, loading, and deploying various ML models.\n",
    "* PIL: Python Image Library, provides some useful functions for displaying images as we work with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's look at some of the functionality available within OpenCV. Below are several functions that highlight some common tasks you might need OpenCV for. If you're not sure what the function is doing, its good to get into the habit of looking up its documentation to double check.\n",
    "\n",
    "For many of the functions below, certain options have been pre-selected. Try to look up the different options that these functions have, and when you'd use a different one. Later on in the notebook, feel free to modify these functions to get some hands-on practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image from a specified path.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path) # OpenCV function for reading an image as a numpy array\n",
    "    if img is None: \n",
    "        print(f\"Error: Image at {image_path} not found\")\n",
    "    return img\n",
    "\n",
    "def display_image(window_name, image):\n",
    "    \"\"\"\n",
    "    Display a specified image.\n",
    "    \"\"\"\n",
    "    cv2.imshow(window_name, image) # Display image\n",
    "    cv2.waitKey(0) # Image will be displayed until the escape key is pressed\n",
    "    cv2.destroyAllWindows() # Stop displaying the image\n",
    "\n",
    "def convert_to_grayscale(image):\n",
    "    \"\"\"\n",
    "    Converts image to grayscale.\n",
    "    \"\"\"\n",
    "    gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # OpenCV has many functions for converting between different colorspaces.\n",
    "    # In this case, COLOR_BGR2GRAY is a conversion from a BGR image to a grayscale image.\n",
    "    # In practice, we'll just call the openCV function in code as opposed to creating a dedicated function like this.\n",
    "    return gray_img\n",
    "\n",
    "def binary_segmentation(image, threshold_value=127):\n",
    "    \"\"\"\n",
    "    Apply a binary segmentation to an image based on an input threshold.\n",
    "    \"\"\"\n",
    "    max_pixel_val = 255\n",
    "\n",
    "    _, thresh_img = cv2.threshold(image, threshold_value, max_pixel_val, cv2.THRESH_BINARY)\n",
    "    # cv2.threshold() supports other thresholding algorithms than just binary. If you're curious, look up\n",
    "    # the functions documentation and try out the other algorithms. When might we use the other types?\n",
    "    return thresh_img\n",
    "\n",
    "def edge_detection(image):\n",
    "    \"\"\"\n",
    "    Detect edges in an image w/ Canny edge detection.\n",
    "    \"\"\"\n",
    "\n",
    "    edges = cv2.Canny(image, 100, 200)\n",
    "    # Question: What do the values 100 and 200 represent? Just to get some practice with documentation, check out opencv's documentation\n",
    "    # for this function to find out!\n",
    "    return edges\n",
    "\n",
    "\n",
    "# For the next two functions, look at their documentation as well. Why might it be useful to extract the contours of an image?\n",
    "\n",
    "def find_contours(image):\n",
    "    \"\"\"\n",
    "    Find the contours in the image.\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    return contours\n",
    "\n",
    "def draw_contours(image, contours):\n",
    "    \"\"\"\n",
    "    Draw the detected contours on the image.\n",
    "    \"\"\"\n",
    "\n",
    "    contour_img = image.copy()\n",
    "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "    return contour_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Functions\n",
    "\n",
    "Below are some functions for common tasks we'd use PyTorch for, such as loading a pre-trained model and getting outputs from it. We won't cover how to actually create and train a model in this notebook since its a bit out of scope, but feel free to ask questions! Later on you will get experience with the creation process of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model_name):\n",
    "    \"\"\"\n",
    "    Load a pre-trained PyTorch model from torchvision.\n",
    "    Try out these three: 'resnet18', 'vgg16', 'densenet121'\n",
    "\n",
    "    If you wish, you can look up each of these models online and see what they're all about.\n",
    "    WARNING: may contain research papers\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name == \"resnet18\":\n",
    "        model = models.resnet18(pretrained=True)\n",
    "\n",
    "    elif model_name == \"vgg16\":\n",
    "        model = models.vgg16(pretrained=True)\n",
    "\n",
    "    elif model_name == \"densenet121\":\n",
    "        model = models.densenet121(pretrained=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not supported.\")\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode instead of train mode\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    When working with neural nets and other ML models, they'll typically have a certain\n",
    "    format for data they can accept. You don't need to get too aquainted with the exact values\n",
    "    we are choosing during this step for now, just know that preprocessing is important.\n",
    "    \"\"\"\n",
    "\n",
    "    input_img = Image.open(image_path).convert(\"RGB\") # Use PIL to read an image in the RGB colorspace\n",
    "\n",
    "    # Next we'll package together all of the preprocessing steps we want to execute using the Compose function\n",
    "    preprocess = transforms.Compose([\n",
    "                transforms.Resize(256), #Resize smallest image dimension to 256 and scale other accordingly\n",
    "                transforms.CenterCrop(224), # Crop the image into a 224x224 square\n",
    "                transforms.ToTensor(), # Convert image to a Tensor. Think of this as an array that models work with\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # For each channel of the input image, normalize values\n",
    "    ])\n",
    "\n",
    "    input_tensor = preprocess(input_img) # Apply our preprocessing to the input image\n",
    "    input_batch = input_tensor.unsqueeze(0) # Convert the 3x224x224 image into a single vector\n",
    "\n",
    "    return input_batch\n",
    "\n",
    "\n",
    "\n",
    "def load_imagenet_labels():\n",
    "    \"\"\"\n",
    "    For this notebook, we'll be using models trained on imagenet, a dataset that features 1000 different classes.\n",
    "\n",
    "    To get a class name for our model outputs as opposed to just the number assigned to the class, we'll\n",
    "    need to load the imagement labels from online.\n",
    "\n",
    "    If you copy the url below into a web browser, you can preview what exactly imagenet can classify.\n",
    "    \"\"\"\n",
    "\n",
    "    # No need to worry about the exact syntax here, but labels to be getting loaded\n",
    "    labels_url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "    import urllib\n",
    "    import json\n",
    "    with urllib.request.urlopen(labels_url) as url:\n",
    "        labels = json.loads(url.read().decode())\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def classify_image(model, input_batch, labels, top_pred=5):\n",
    "    \"\"\"\n",
    "    Perform classification on an input image using a pre-trained model and print the top predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad(): # Improves classification speed by skipping model gradient computation\n",
    "        output = model(input_batch) # Get classification from our model\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(output[0], dim=0) # Convert model output into probabilities\n",
    "\n",
    "    top_probs, top_indices = torch.topk(probs, top_pred)\n",
    "\n",
    "    for i in range(top_pred): # Print out our top predictions\n",
    "        label = labels[top_indices[i]]\n",
    "        prob = top_probs[i].item()\n",
    "        print(f\"{label}: {prob * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation Time!!\n",
    "\n",
    "Now that we've covered a few functions above, let's spend some time trying to process an image of our own. This part of the notebook is pretty open-ended, so feel free to download your own images to try out some of the processing above.\n",
    "\n",
    "To get things stared, I'd recommend trying to find an image with an object distinct from its background. Try to segment this object, find only the contours of the segmented portion, and then overlaying these contours onto the original image!\n",
    "\n",
    "Once you've done that, try out classifying the image, and then overlaying the predicted class label ontop of the image with the contours. To draw text over an image, check out the [cv2.putText() function.](https://www.geeksforgeeks.org/python-opencv-cv2-puttext-method/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer vision time woop woop!\n",
    "\n",
    "# Try out with the provided coffee_cup image, or find your own!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
